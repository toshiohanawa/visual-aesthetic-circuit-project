---
alwaysApply: true
---
The project runs entirely on DGX Spark with CUDA GPUs.
Always assume device = "cuda" and do not generate CPU, MPS, or Mac execution paths.
Do not include environment fallback logic (no CPU/MPS checks).
All computation happens on DGX Spark only.

The Python environment is managed exclusively with uv on the DGX machine.
Do not suggest or generate conda, venv, poetry, or Mac-side environments.

Model execution must use local GPU models only.
Do not use API endpoints (OpenAI, Gemini, etc.).
Load models with local from_pretrained().

Prefer research-oriented code: explicit logic, deterministic seeds, .eval(), torch.no_grad(), saved activations, and clear directory structure.

Use the following file layout:

notebooks → notebooks/

SAE code → Phase1/

Patching code → Phase2/

Steering code → Phase3/

checkpoints/activations → models/

CUDA version is not fixed yet.
Phase 0 will determine compatible versions of PyTorch, bitsandbytes, Triton, FlashAttention, and related libraries.
Do not assume or require the latest CUDA version.

Docker suggestions should use GPU-enabled images and the NVIDIA Container Toolkit.
Avoid cross-platform logic and Mac-specific paths.